{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>oii</p>"},{"location":"Data%20Engineer/apacheSpark/","title":"Apache Spark","text":""},{"location":"Data%20Engineer/apacheSpark/#o-que-e","title":"O que \u00e9","text":"<p>\u00c9 uma engine de processamento em ambiente distribu\u00eddo</p> <p>Imagine v\u00e1rios clusters de computadores trabalhando juntos de forma interconectada e distribuindo seu conjunto de trabalho em v\u00e1rios n\u00f3s nestas m\u00e1quinas, com sua CPU e mem\u00f3ria compartilhadas com o objetivo de aumentar seu processamento ou disponibilidade do ambiente, resultando em uma \u00f3tima escalabilidade</p> <p>Usando somente uma m\u00e1quina, voc\u00ea se limita aos recursos computacionais, o poder de processamento daquela \u00fanica m\u00e1quina</p> <p>\u00c9 poss\u00edvel usar o framework do Spark para acessar v\u00e1rios tipos de bancos de dados, em batch e em streaming, com Python at\u00e9 Java, sendo ele extremamente vers\u00e1til</p> <p></p> <p></p>"},{"location":"Data%20Engineer/apacheSpark/#spark-vs-outras-ferramentas","title":"Spark vs outras ferramentas","text":"<p>Diferente do Hadoop, o Spark n\u00e3o trabalha fazendo o processamento em disco, mas sim em mem\u00f3ria, aumetando ainda mais o tempo de processamento</p> <p>Mas o Hadoop ainda \u00e9 bastante usado por ter seu file system compartilhado com o HDFS, apesar de seu uso ser mais dif\u00edcil e somente possuir processamento em batch</p> <p></p> <p>Os dados processados pelo Pandas, os dataframes, ficam alocados em uma mem\u00f3ria standalone, diferete do Spark que o faz na mem\u00f3ria de v\u00e1rias m\u00e1quinas, tanto em leitura, quanto em escrita</p> <p></p>"},{"location":"Data%20Engineer/apacheSpark/#a-dupla-apache-pyspark-e-parquet","title":"A dupla Apache Pyspark e Parquet","text":"<p>Formato de dados desenvolvido de forma otimizada a trabalhar com BigData, armazenando seus dados em formato de coluna, os comprimindo, codificando, e criando parti\u00e7\u00f5es f\u00edsicas, desenvolvido pela Apache</p> <p></p> <p>A consulta \u00e9 ent\u00e3o otimizada pois os tipos de dados est\u00e3o pr\u00f3ximos, e n\u00e3o embaralhados, al\u00e9m de seu armazenamento ser drasticamente reduzido por substitui\u00e7\u00e3o de redund\u00e2ncia, ganhando espa\u00e7o em disco e tempo de processamento</p> <p></p> <p>Essa diferen\u00e7a de performance \u00e9 imperativa, ainda mais em cloud que o custo \u00e9 por tempo de processamento, ciclo de CPU e armazenamento em disco</p> <p></p> <p></p>"},{"location":"Data%20Engineer/apacheSpark/#meu-primeiro-pyspark","title":"Meu primeiro Pyspark","text":"<p>Ser\u00e1 usado o Databricks Comunnity, criar conta l\u00e1 portanto, e como exemplo a base de dados train.csv do Kaggle: Housing Prices</p> <p>Dever\u00e1 ser feito o upload do arquivo ou pela aba Catalog -&gt; Create Table, ou j\u00e1 no arquivo .dbc do Databricks, com File -&gt; Upload data to DBFS...</p> <pre><code># Localiza\u00e7\u00e3o do arquivo e seu tipo\nfile_location = \"/FileStore/tables/train.csv\"\nfile_type = \"csv\"\n\n# Op\u00e7\u00f5es do CSV\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# O padr\u00e3o de c\u00f3digo Spark para CSV, outros tipos desconsiderar\ndf = spark.read.format(file_type) \\\n    .option(\"inferSchema\", infer_schema) \\\n    .option(\"header\", first_row_is_header) \\\n    .option(\"sep\", delimiter) \\\n    .load(file_location)\n</code></pre> <p>A op\u00e7\u00e3o <code>header</code> determinar\u00e1 a primeira linha do dataset como sendo o cabe\u00e7alho</p> <p>A op\u00e7\u00e3o <code>inferSchema</code> far\u00e1 a infer\u00eancia dos tipos de dados das colunas automaticamente, fazendo com que seja implementado outro Spark Job somente para este trabalho, deixando este processo inicial um pouco mais lento</p> <p>Caso opte por passar os tipos das colunas manualmente, ganhando tempo de processamento, \u00e9 poss\u00edvel utilizar somente a op\u00e7\u00e3o <code>schema</code></p> <pre><code>from pyspark.sql.types import StringType, DoubleType # (1)!\n\ncustomSchema = StructType(Array(\n    StructField(\"IDGC\", StringType, True),        \n    StructField(\"SEARCHNAME\", StringType, True),\n    StructField(\"PRICE\", DoubleType, True),\n    # etc...\n))\n\ndf = spark.read.format(file_type) \\\n    .option(\"schema\", customSchema) \\\n    .option(\"header\", first_row_is_header) \\\n    .option(\"sep\", delimiter) \\\n    .load(file_location)\n</code></pre> <ol> <li>Para saber mais dos tipos de dados: https://spark.apache.org/docs/latest/sql-ref-datatypes.html</li> </ol> <p>\u00c9 poss\u00edvel acompanhar o processo do cluster Spark na aba Compute -&gt; Spark UI</p> <p>Qualquer comando do Spark, mesmo feito em Python, \u00e9 considerado um c\u00f3digo SQL, logo, uma query</p> <p>Para traduzir o dataframe, seja formato xlsc ou csv, para o formato parquet, faz-se da seguinte forma (pode demorar um pouco devido a escrita dos metadados):</p> <pre><code>df.write.format(\"parquet\")\\\n.mode(\"overwrite\")\\\n.save(\"/FileStore/tables/processing/df-parquet-file.parquet\")\n</code></pre> <p>O modo <code>overwrite</code> identifica caso haja outro arquivo parquet com o mesmo nome e o subscreve for\u00e7adamente</p> <p>Para fazer a leitura:</p> <pre><code>df_parquet = spark.read.format(\"parquet\")\\\n.load(\"/FileStore/tables/processing/df-parquet-file.parquet\")\n</code></pre> <p></p>"},{"location":"Data%20Engineer/apacheSpark/#o-paradigma-de-performance-tuning-no-spark","title":"O paradigma de Performance Tuning no Spark","text":"<p>someday...</p>"},{"location":"Data%20Engineer/dataEngineer/","title":"Quem \u00e9","text":""},{"location":"Data%20Engineer/dataEngineer/#o-que-faz","title":"O que faz?","text":"<p>O Engenheiro de Dados tem a fun\u00e7\u00e3o de projetar, desenhar, definir e construir sistemas e processos para solu\u00e7\u00f5es de dados</p> <p>Envolvido na coleta, transforma\u00e7\u00e3o, entrega, armazenamento, an\u00e1lise e deploy destes dados em escala Conhecimento da solu\u00e7\u00e3o de ponta-a-ponta com as data pipelines</p> <p>Tra\u00e7ar or\u00e7amentos, planejamentos, mais efetivos, envolvido na parte de projetos e neg\u00f3cios</p> <p>Cuida mais da parte de BI, ETL e ELT orientados a BigData</p> <p>Realizar a montagem de ecossistemas como Data Lakes, Warehouses, Lakehouses, etc</p> <p>Cuida mais das solu\u00e7\u00f5es com o \"back-end\", escolhendo a arquitetura da infraestrutura</p> <p>Garante que o requisito da aplica\u00e7\u00e3o de neg\u00f3cios aconte\u00e7a</p> <p>Define as fontes dos dados, como ser\u00e3o coletados, a frequ\u00eancia em que ser\u00e3o coletados, como eles ser\u00e3o geridos, etc</p> <p>Voc\u00ea decide os formatos dos arquivos: JSON, Parquet, CSV, SQL, etc</p>"},{"location":"Data%20Engineer/dataEngineer/#big-data","title":"Big Data","text":"<p>Deixamos de trabalhar somente com dados estruturados para trabalhar tamb\u00e9m com dados semi estruturados e n\u00e3o estruturados, com novos formatos de dados</p> <p>Os 3 V's: Velocidade, Volume e Variedade</p> <p></p>"},{"location":"Data%20Engineer/dataEngineer/#supervisao-de-aplicacoes","title":"Supervis\u00e3o de Aplica\u00e7\u00f5es","text":"<p>\u00c9 interessante guardar informa\u00e7\u00f5es de scripts e pipelines - a exemplo de status de ETLs - por meio de logs, erros, tempo de dura\u00e7\u00e3o, custo do processamento</p>"},{"location":"Data%20Engineer/dataEngineer/#pipelines","title":"Pipelines","text":"<p>Defini\u00e7\u00e3o de ambientes e arquiteturas, como on-premise e cloud ou h\u00edbrido</p> <p>Especificar as tecnologias de armazenamento, como Data Lakes e Data Warehouses</p> <p>Selecionar as formas de processamento como Streaming ou Clusters</p> <p>Definir o monitoramento e padr\u00f5es de seguran\u00e7a e integra\u00e7\u00e3o com outros sistemas</p>"},{"location":"Data%20Engineer/dataEngineer/#ferramentas-de-nuvem","title":"Ferramentas de nuvem","text":"<ul> <li>Databricks</li> <li>Confluent</li> <li>Cloudera</li> <li>Horton Works</li> <li>Microsoft Azure</li> <li>Google Cloud Plataform</li> <li>AWS</li> <li>Ecossistema Hadoop</li> </ul>"},{"location":"Data%20Engineer/dataLake/","title":"Data Lake","text":""},{"location":"Data%20Engineer/dataLake/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Surgidos em 2011</p> <p>N\u00e3o \u00e9 uma tecnologia, n\u00e3o \u00e9 um protocolo, n\u00e3o \u00e9 um framework, \u00e9 somente um conceito, algo abstrato. Outra coisa \u00e9 a sua forma de se implementar </p> <p>Um reposit\u00f3rio \u00fanico para armazenar todos os formatos de dados, sendo eles estruturados, semi-estruturados e at\u00e9 n\u00e3o-estruturados</p> <p>Mais necess\u00e1rio para a finalidade de testar um modelo de ML, descobrir as features do modelo, necessitando dos formatos estarem brutos, n\u00e3o processados</p> <p>Seus dados s\u00e3o transformados apenas quando s\u00e3o necess\u00e1rios para an\u00e1lises, por meio de aplica\u00e7\u00f5es de rotinas por exemplo, esta a\u00e7\u00e3o chamada de esquema para leitura</p> <p>N\u00e3o \u00e9 necess\u00e1rio fazer a migra\u00e7\u00e3o dos dados para outro sistema, sendo a sua gera\u00e7\u00e3o de relat\u00f3rios mais uma quest\u00e3o ad hoc</p> <p>Pode ser trabalhado com dados em batch ou em streaming</p> <ul> <li>Batch: Grande carga de dados que chegam em tantos per\u00edodos espec\u00edficos de tempo, como a cada dia, cada 30 min, etc, sendo ele programado para acontecer com frequ\u00eancia, portanto, um processo em lote que \u00e9 processado tudo de uma vez, exemplo de uso seria a arquitetura Lambda a seguir</li> </ul> <p></p> <ul> <li>Streaming: Pequenas por\u00e7\u00f5es de dados que chegam em tempo real, sendo processados por fun\u00e7\u00f5es de MapReduce para organizar os dados embaralhados, exemplo desse uso seria a arquitetura Kappa e tamb\u00e9m a Lambda</li> </ul> <p></p> <p>Seu objetivo \u00e9 entregar r\u00e1pidos insights, coma menor burocracia e restri\u00e7\u00e3o poss\u00edvel</p> <p>O principal usu\u00e1rio \u00e9 o Cientista de Dados, capaz de tratar e processar os dados n\u00e3o estruturados a bel prazer. Logo, fornece principalmente dados para AI e modelos de Machine Learning</p> <p>O modelo de schema \u00e9 definido no momento de leitura, n\u00e3o existindo uma tabela r\u00edgida com campos bem determinados</p> <p>Deve-se ter muito cuidado para n\u00e3o virar um Data Swamp, para isso, boas pr\u00e1ticas devem ser adotadas, como sistema de controle de acesso, controle de cotas por zona de gerenciamento, como landing zones, process zones, sendo estas, exemplos de uma arquitetura medallion ou multihop, com os dados brutos entrando na zona Bronze os pr\u00e9-processados no Prata e o dado final no Ouro, estipula\u00e7\u00e3o da periodicidade da ingest\u00e3o de dados, n\u00e3o armazenar dados in\u00fateis, defini\u00e7\u00e3o de regras de neg\u00f3cios, diversas fontes de dados com diversas pipelines, com fluidez de dados, garantir os metadados (os dados sobre os dados) e etc</p> <p>Basicamente, \u00e9 necess\u00e1rio ter governan\u00e7a e rotinas de limpeza</p> <p>Para evitar uma bagun\u00e7a total dos datasets, \u00e9 extremamente importante ter a divis\u00e3o de zonas dentro do Data Lake, cada uma com um prop\u00f3sito (landing -&gt; process - curated)</p> <p></p> <p></p>"},{"location":"Data%20Engineer/dataLake/#solucoes-do-mercado","title":"Solu\u00e7\u00f5es do Mercado","text":"<p>Importante frizar que todas os seguintes exemplos s\u00e3o solu\u00e7\u00f5es e servi\u00e7os em cloud</p>"},{"location":"Data%20Engineer/dataLake/#amazon-bucket-s3","title":"Amazon Bucket S3","text":"<p>Uma solu\u00e7\u00e3o para armazenamento de objetos, onde a partir dele, h\u00e1 v\u00e1rias outras solu\u00e7\u00f5es integradas para fazer a gest\u00e3o dos dados</p> <p>Escal\u00e1vel, seguro, com um custo vi\u00e1vel</p> <p>Os Buckets (reposit\u00f3rios) podem ter configura\u00e7\u00f5es de regi\u00e3o, replica\u00e7\u00e3o diferentes, de tags, metadados, etc</p> <p></p> <p>Solu\u00e7\u00f5es integradas para:</p> <ul> <li>Pesquisa e Cat\u00e1logo dos dados</li> <li>Ingest\u00e3o de dados, com capta\u00e7\u00e3o e coleta de dados</li> <li>Fun\u00e7\u00f5es de seguran\u00e7a, como criptografias, permiss\u00f5es</li> <li>Interfaces de acesso de usu\u00e1rio</li> <li>Servi\u00e7oes de analytics</li> </ul> <p></p>"},{"location":"Data%20Engineer/dataLake/#amazon-lake-formation","title":"Amazon Lake Formation","text":"<p>Solu\u00e7\u00e3o mais especializada em Data Lakes, sendo um pouco mais completa que o S3, at\u00e9 podendo consumir dados do mesmo, consumindo tamb\u00e9m de umbanco de dados relacional e n\u00e3o relacionais tamb\u00e9m</p> <p>Pode possuir solu\u00e7\u00f5es de crawlers, ETL e prepara\u00e7\u00e3o de dados integrados, cataloga\u00e7\u00e3o dos dados, configura\u00e7\u00f5es de seguran\u00e7a e controles de acesso</p> <p>Podendo tamb\u00e9m integrar dentro da Amazon Athena (solu\u00e7\u00e3o de analytics da AWS), Amazon Redshift e Amazon EMR, e etc</p> <p>O armazenamento pode at\u00e9 ser feito dentro do Bucket S3, mas sua gest\u00e3o do Data Lake seria feita dentro do Lake Formation</p> <p></p>"},{"location":"Data%20Engineer/dataLake/#azure-data-lake","title":"Azure Data Lake","text":"<p>Data Lake Store prov\u00ea solu\u00e7\u00f5es de armazenamento para dados estruturados, semi-estruturados e n\u00e3o-estruturados</p> <p>Data Lake Analytics para solu\u00e7\u00f5es de analytics (pasme) com ampla gama de linguagens de programa\u00e7\u00e3o como ferramentas dispon\u00edveis, tais como R, Python, C#, T-SQL e .NET</p> <p>HDInsight para processamento distribu\u00eddo, com um amplo leque de ferramentas tamb\u00e9m, como Java e Scala</p> <p></p>"},{"location":"Data%20Engineer/dataLake/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Possui servi\u00e7o de armazenamento, servi\u00e7os de pipelines com GC Fusion, Big Query para analytics, orquestra\u00e7\u00f5es com o Cloud Composer, monitoramento com Logging e Cloud Data Catalog para cataloga\u00e7\u00e3o dos dados (pasme)</p> <p></p> <p>Geralmente os dados de sensores s\u00e3o n\u00e3o estruturados ou semi-estruturados, por se tratarem de JSON</p> <p>Dados gerados on-premise</p> <ul> <li> <p>Cloud: Os servi\u00e7os est\u00e3o al\u00e9m do seu computador/servidor local, est\u00e3o na nuvem, sendo gerenciados ou armazenados em um outro local de talvez uma outra empresa, sendo seu custo a exemplo de hardwares, seguran\u00e7a e backups diminu\u00eddos pois essa bronca est\u00e1 com a fornecedora da nuvem, com voc\u00ea pagando somente com o que consome</p> </li> <li> <p>On-premise: S\u00e3o os sistemas, servidores internos de uma empresa, implanta\u00e7\u00f5es internas da TI que gerenciam toda a infraestrutura, \u00e9 o local. Por exemplo, voc\u00ea compraria o hardware do servidor e faria a manuten\u00e7\u00e3o dele, onde tamb\u00e9m configuraria e atualizaria os sistemas operacionais nos quais seu software \u00e9 executado, al\u00e9m de instalar e atualizar todos os complementos e plug-ins necess\u00e1rios, gastando tamb\u00e9m com parte el\u00e9trica e refrigera\u00e7\u00e3o, sendo sua implementa\u00e7\u00e3o e gerenciamento de infraestrutura um tanto quanto complexos</p> </li> <li> <p>SaaS Software as a Service: Todas as responsabilidades do de cima, sendo um software como o nome sugere, passam a ser do prestador do servi\u00e7o de nuvem, seguran\u00e7a, confidencialidade, manuten\u00e7\u00e3o, etc</p> </li> </ul> <p>O GCP possui tamb\u00e9m servi\u00e7os para dados em streaming, batch, gerenciamento do Data Lake por linha de comando com o gsutil, assim como data mining, machine learning, etc, tudo integrado ao seu sistema de armazenamento</p> <p></p>"},{"location":"Data%20Engineer/dataLake/#solucoes-open-source","title":"Solu\u00e7\u00f5es Open Source","text":"<p>Solu\u00e7\u00f5es que podem ser implementadas em seu ambiente on-premise gastando muito menos do que gastaria com uma AWS da vida por causa de suas assinaturas e planos</p>"},{"location":"Data%20Engineer/dataLake/#minio","title":"Minio","text":"<p>Implementa o protocolo S3 da AWS, podendo ser orquestrado tamb\u00e9m com Kubernetes</p> <p>Solu\u00e7\u00e3o excelente para cloud h\u00edbrida</p> <p></p>"},{"location":"Data%20Engineer/dataLake/#lake-fs","title":"Lake FS","text":"<p>Pode ser integrado tamb\u00e9m com diversas outras solu\u00e7\u00f5es e protocolos</p> <p>Faz a gest\u00e3o do versionamento do Data Lake a n\u00edvel de c\u00f3digo</p> <p></p>"},{"location":"Data%20Engineer/dataLake/#hadoop-file-system","title":"Hadoop File System","text":"<p>Famoso HDFS</p> <p>N\u00e3o recomend\u00e1vel usar somente ele pois perde solu\u00e7\u00f5es como sistema de controle de acesso, controle de cotas por zona, etc</p>"},{"location":"Data%20Engineer/dataLakehouse/","title":"Data Lakehouse","text":""},{"location":"Data%20Engineer/dataLakehouse/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Surgidos em 2020 devido a alta demanda de algoritmos de IA e Machine Learning para dados n\u00e3o-estruturados</p> <p>O que era mais usado era o uso de v\u00e1rios DW e um DL , sendo dificultoso manter diferentes solu\u00e7\u00f5es para v\u00e1rios ambientes</p> <p>Uma jun\u00e7\u00e3o do Data Lake (baixo custo) com Data Warehouse (alta gest\u00e3o), com um ambiente flex\u00edvel e com confiabilidade</p>"},{"location":"Data%20Engineer/dataMart/","title":"Data Mart","text":""},{"location":"Data%20Engineer/dataMart/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>S\u00e3o solu\u00e7\u00f5es mais especializadas e condensadas de determinadas \u00e1reas e setores no formato de Data Warehouses s\u00f3 que em miniatura, sendo eles mais um subconjunto de dados em vez de dados corporativos completos, com o prop\u00f3sito de atender \u00e0s necessidades de unidades de neg\u00f3cios, fun\u00e7\u00f5es ou departamentos muito espec\u00edficos</p> <p>Reduz portanto o risco de perda de dados, bem como os custos e o tempo de implementa\u00e7\u00e3o, sendo bem eficazes para fornecer suporte de decis\u00e3o r\u00e1pido e consistente para uma Decision Support System DSS (Sitema de Suporte \u00e0 Decis\u00e3o)</p> <p>Podem estar presentes tanto em DW, quanto em DL</p> <p></p>"},{"location":"Data%20Engineer/dataWarehouse/","title":"Data Warehouse","text":""},{"location":"Data%20Engineer/dataWarehouse/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Surgidos no final da d\u00e9cada de 80</p> <p>S\u00e3o CAROS, LENTOS e COMPLEXOS pois trabalham com bases de dados massivas, feitos para queries escal\u00e1veis</p> <p>No levantamento de requisitos da modelagem de um DW, j\u00e1 \u00e9 definido antecipadamente qual assunto o DW vai tratar</p> <p>Diferentes dos Data Lakes, os Data Warehouses s\u00e3o solu\u00e7\u00f5es mais controladas e confi\u00e1veis, uma \"\u00fanica fonte de verdade de dados\", com os mesmos sendo bem estruturados pelas modifica\u00e7\u00f5es do processo de ETL e n\u00e3o vol\u00e1teis</p> <p>Sua principal ideia \u00e9 reduzir o trabalho manual para que seja facilitado o foco nas pesquisas e consultas, reaproveitando o processamento e assim melhorando a performance para a gera\u00e7\u00e3o de BI e relat\u00f3rios. Logo, seus principais usu\u00e1rios s\u00e3o da \u00e1rea de BI, gestores, gerentes </p> <p>Muito utilizado devido a suas transa\u00e7\u00f5es ACID</p> <ul> <li>ACID Atomicity, Consistency, Isolation, Durability: Uma garantia de que a transa\u00e7\u00e3o nos bancos de dados foi bem feita, co uma transa\u00e7\u00e3o n\u00e3o interferindo na outra. Ela come\u00e7a e finaliza, n\u00e3o podendo ficar no meio (mandei 100 reais e chegou 50?)</li> </ul> <p>Seus schemas s\u00e3o feitos no momento de escrita definidos com anteced\u00eancia, sendo necess\u00e1rio o conhecimento das tabelas, para otimizar as consultas SQL</p> <p>Todos os conceitos de uma DW s\u00e3o v\u00e1lidos para a constru\u00e7\u00e3o de uma arquitetura mais moderna como um Data Lakehouse</p> <p>\u00c9 melhor n\u00e3o fazer um DW de tudo e disponibilizar as visualiza\u00e7\u00f5es, mas sim, disponibilizar os dados atrav\u00e9s de Data Marts</p> <p>O DW n\u00e3o chega a ser somente uma \"vitrine\" do banco de dados pois dentro dele ser\u00e1 feita a limpeza e c\u00e1lculo de KPIs que podem at\u00e9 nem existir em um DM, necessitando da jun\u00e7\u00e3o de mais de um, como por exemplo o de marketing e de produto. N\u00e3o \u00e9 s\u00f3 exp\u00f4r o dado de uma maneira melhor, mas gerar ainda mais informa\u00e7\u00e3o</p> <ul> <li>KPI: </li> </ul>"},{"location":"Data%20Engineer/dataWarehouse/#eu-preciso-momento-storytelling","title":"Eu preciso? (momento storytelling)","text":"<p>Chega na sua empresa e pergunta pro analista quais produtos foram vendidos no dia anterior e para quais clientes estes mesmos foram vendidos, dependendo da dificuldade de responder, urge a necessidade de um DW</p> <p>J\u00e1 pensou participar de uma reuni\u00e3o com os times de vendas, cada um com um valor de quanto vendeu ontem, marketing, financeiro, comercial, etc</p> <p>Para qu\u00ea eu preciso disso? N\u00e3o seria melhor s\u00f3 conectar diretamente no sistema?</p> <p></p>"},{"location":"Data%20Engineer/dataWarehouse/#imagina-conectar-daqui","title":"Imagina conectar daqui","text":"<p>\u00c9 um sistema transacional r\u00e1pido, n\u00e3o foi feito para consultas anal\u00edticas</p> <p>Sem um DW ent\u00e3o, ser\u00e1 extra\u00eddo de um sistema ERP uma planilha de vendas para que ent\u00e3o possa ser visualizado o relat\u00f3rio do dia anterior, fazendo gr\u00e1ficos, procv, etc</p> <p></p>"},{"location":"Data%20Engineer/dataWarehouse/#diagrama-de-fluxo-retirado-da-supletiva-do-data-hackers-com-luciano-vasconcelos","title":"Diagrama de Fluxo retirado da supletiva do Data Hackers com Luciano Vasconcelos","text":"<p>E se precisar escalar e n\u00e3o ter de fazer somente com o sistema ERP, mas tamb\u00e9m o de CRM? E se o relat\u00f3rio ao inv\u00e9s de ser em um dia, precisar ser feito em uma hora? E se a venda de Ecommerce for um outro sistema com JSON, e o outro ERP for legado com XML?</p> <ul> <li> <p>ERP Enterprise Resourse Planning: O Planejamento dos Recursos da Empresa</p> </li> <li> <p>CRM Customer Relationship Management: O Sistema Integrado de Gest\u00e3o Empresarial</p> </li> </ul> <p>S\u00f3 as empresas que crescem possuem volume e faturamento para precisar de um DW, para conseguir automatizar, profissionalizar e diminuir o n\u00famero de falhas com processos de ETL, tudo com o intuito de unir os sistemas em um lugar para os analistas consumirem </p> <p>A pior maneira de come\u00e7ar um projeto de DW \u00e9 conectando todas as fontes de dados da empresa. O que d\u00e1 dinheiro n\u00e3o \u00e9 integrar, \u00e9 gerar os relat\u00f3rios, as an\u00e1lises</p> <p>A grande quest\u00e3o da modelagem \u00e9 que quem vai usar o seu modelo, n\u00e3o \u00e9 voc\u00ea, \u00e9 o usu\u00e1rio final, se n\u00e3o foi feito algo f\u00e1cil, seu projeto vai desmoronar</p>"},{"location":"Data%20Engineer/databricks/","title":"Databricks","text":"<p>Solu\u00e7\u00e3o em cloud com o objetivo de processar clusters de m\u00e1quinas Spark, com diversar solu\u00e7\u00f5es propriet\u00e1rias (apesar da maioria serem pagas)</p> <p>Grandes empresas que usam clouds DEVEM se tornar parceiras do Databricks, como o Google, AWS, Azure, etc</p> <p>Usa do DBFS (Databricks File System) para armazenamento interno</p>"},{"location":"Data%20Engineer/etlElt/","title":"As etapas do processamento dos dados","text":""},{"location":"Data%20Engineer/etlElt/#o-que-e-etl-e-elt","title":"O que \u00e9 ETL e ELT?","text":"<p>S\u00e3o um conceito de passo-a-passo a se seguirem conforme uma solu\u00e7\u00e3o de dados \u00e9 proposta na necessidade do seu neg\u00f3cio</p> <ul> <li> <p>E significa Extract (Extrair), \u00e9 a etapa de obten\u00e7\u00e3o dos dados</p> </li> <li> <p>T significa Transform (Transformar), \u00e9 a etapa de processamento, aplica\u00e7\u00e3o de rotinas</p> </li> <li> <p>L significa Load (Carregar), \u00e9 a etapa de armazenar os dados obtidos em uma fonte de dados</p> </li> </ul> <p>O processo de ELT tem tudo a ver com os Data Lakes, visto que armazenam dados semi e n\u00e3o estruturados, podendo ou n\u00e3o serem processados depois</p> <p>J\u00e1 o processo de ETL \u00e9 correspondente com as Data Warehouses, pois necessitam dos seus dados tratados, logo, estruturados</p>"},{"location":"Data%20Engineer/pipelines/","title":"Pipelines","text":"<p>Um pipeline de dados, ou datapipeline, \u00e9 uma s\u00e9rie de etapas de processamento de dados. Sendo que, cada etapa fornece uma sa\u00edda que \u00e9 a entrada para a pr\u00f3xima etapa. Isso vai acontecendo at\u00e9 que o pipeline seja conclu\u00eddo. Al\u00e9m disso, tamb\u00e9m podem existir etapas independentes a serem executadas em paralelo</p> <p>A maioria dos pipelines possuem tr\u00eas elementos principais: a origem, uma ou mais etapas de processamento e o destino. Mas os pipelines de dados podem ser arquitetados de v\u00e1rias maneiras diferentes e tudo vai depender do caso de uso por meio das DAGs, orquestra\u00e7\u00e3o de pipeline e VMs </p> <ul> <li> <p>Directed Acyclic Graph DAG (Gr\u00e1fico Ac\u00edclico Direcionado): Define as regras do que ser\u00e1 efetivamente orquestrado no seu pipeline, refletindo suas rela\u00e7\u00f5es e depend\u00eancias, sendo sua sigla: Grafos para a conex\u00e3o dos n\u00f3s; Direcionado indicando que o fluxo de trabalho se d\u00e1 apenas em uma dire\u00e7\u00e3o; e Ac\u00edclico: significa que a execu\u00e7\u00e3o n\u00e3o entrar\u00e1 em um la\u00e7o de repeti\u00e7\u00e3o</p> </li> <li> <p>Virtual Machine VM: </p> </li> </ul> <p></p> <p>Ser\u00e1 necess\u00e1rio definir </p>"},{"location":"Data%20Engineer/pipelines/#apache-airflow","title":"Apache Airflow","text":"<p>Ferramenta open source, escrita em Python, criada pelo Airbnb em 2014 e atualmente faz parte da Apache Software Foundation. Trata-se de um orquestrador de fluxos, ou seja, nos permite decidir em qual momento e em quais condi\u00e7\u00f5es nosso programa ir\u00e1 rodar. \u00c9 utilizada principalmente para cria\u00e7\u00e3o, monitoramento e agendamento de pipeline de dados de forma program\u00e1tica</p> <p>Cont\u00e9m algumas bibliotecas que s\u00f3 funcionam no Linux. Dessa forma, solu\u00e7\u00f5es alternativas para usu\u00e1rios(as) de Windows, como m\u00e1quinas virtuais ou Docker, s\u00e3o necess\u00e1rias para uso totalmente funcional dessa ferramenta</p> <ul> <li>Task (tarefa): Unidade mais b\u00e1sica de um DAG, usada para implementar uma determinada l\u00f3gica na pipeline, s\u00e3o definidos pela instancia\u00e7\u00e3o de um Operator -DAG ou Job (trabalho): Conjunto de tarefas</li> <li>Operators (operadores): Blocos de constru\u00e7\u00e3o de um DAG, contendo a l\u00f3gica de como os dados s\u00e3o processados em uma data pipeline, sendo classes Python por de baixo dos panos. Quando uma inst\u00e2ncia de um Operator \u00e9 criado em um DAG com os par\u00e2metros necess\u00e1rios, essa inst\u00e2nca do Operator passa a ser uma Task. Caso n\u00f5 exista um operador para seu caso de uso, \u00e9 poss\u00edvel criar o seu pr\u00f3prio</li> </ul> <p>Um Operador possui 3 caracter\u00edsticas principais:</p> <ul> <li>Idempot\u00eancia: independentemente de quantas vezes uma tarefa for executada com os mesmos par\u00e2metros, o resultado final deve ser sempre o mesmo</li> <li>Isolamento: a tarefa n\u00e3o compartilha recursos com outras tarefas de qualquer outro operador</li> <li>Atomicidade: a tarefa \u00e9 um processo indivis\u00edvel e bem determinado</li> </ul> <p></p>"},{"location":"Data%20Engineer/pipelines/#diagrama-do-airflow","title":"Diagrama do Airflow","text":"<p>O Airflow possui 4 componentes principais que devem estar em execu\u00e7\u00e3o para que ele funcione corretamente:</p> <ul> <li>Web Server: Servidor feito em Flask, por onde \u00e9 acessado sua interface. Permite inspecionar, acionar e acompanhar o comportamento dos DAGs e suas tarefas</li> <li>Pasta de arquivos DAG: armazena os arquivos DAGs criados. Ela \u00e9 lida pelo agendador e executor</li> <li>Scheduler (agendador): Respons\u00e1vel pelo agendamento da execu\u00e7\u00e3o das tarefas dos DAGs, ent\u00e3o ele determina quais tarefas ser\u00e3o realizadas, onde ser\u00e3o executadas e em qual ordem isso acontecer\u00e1 para o executor</li> <li>Banco de Dados: Serve para armazenar todos os metadados referentes aos DAGs e suas tarefas. Sendo assim, ele registra o hor\u00e1rio em que as tarefas foram executadas, quanto tempo cada task levou para ser realizada e o estado de cada uma - se foram executadas com sucesso ou falha, e outras informa\u00e7\u00f5es relacionadas</li> <li>Executor: Mecanismo de execu\u00e7\u00e3o das tarefas. Ou seja, ele \u00e9 respons\u00e1vel por descobrir quais recursos ser\u00e3o necess\u00e1rios para executar essas tasks. Possui v\u00e1rios executores, mas somente um \u00e9 utilizado por vez</li> </ul> <p>Um exemplo de componente situacional seria:</p> <ul> <li>Worker: Processo que executa as tarefas conforme definido pelo executor. Dependendo do executor escolhido, voc\u00ea pode ou n\u00e3o ter workers (trabalhadores) como parte da infraestrutura do Airflow.</li> </ul>"},{"location":"Data%20Engineer/pipelines/#meu-primeiro-dag-airflow","title":"Meu Primeiro DAG Airflow","text":"<pre><code>from airflow.models import DAG\nimport pendulum\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.operators.bash import BashOperator\n\nwith DAG(\n    'meu_primeiro_dag',\n    start_date=pendulum.today('UTC').add(days=-N),\n    schedule_interval='@daily'\n) as dag:\n    tarefa_1 = EmptyOperator(task_id = 'tarefa_1')\n    tarefa_2 = EmptyOperator(task_id = 'tarefa_2')\n    tarefa_3 = EmptyOperator(task_id = 'tarefa_3')\n    tarefa_4 = BashOperator(\n        task_id = 'cria_pasta',\n        bash_command = 'mkdir -p \"home/millenagena/Documents/airflowalura/pasta\" '\n    )\n\n    tarefa_1 &gt;&gt; [tarefa_2, tarefa_3]\n    tarefa_3 &gt;&gt; tarefa_4\n</code></pre>"},{"location":"Data%20Engineer/pipelines/#azure-data-factory","title":"Azure Data Factory","text":"<p>Solu\u00e7\u00e3o paga para orquestra\u00e7\u00e3o de data pipelines da Microsoft, estando integrada com seus outros servi\u00e7os</p> <p></p>"},{"location":"Data%20Engineer/pipelines/#aws-glue","title":"AWS Glue","text":"<p>Integrado tamb\u00e9m com as solu\u00e7\u00f5es da Amazon</p>"},{"location":"Data%20Engineer/pipelines/#google-cloud-composer","title":"Google Cloud Composer","text":""}]}